# ğŸ­ Industrial Data Crawler Pro

**Single-file, enterprise-grade web data extraction system**

## âœ¨ Highlights

- **Self-Contained** - Everything in one `crawler_app.py` file (700+ lines)
- **Modern UI** - Two-panel layout with live stats
- **Enhanced Extraction** - Tables, metadata, links, structured data
- **Multi-Format Export** - JSON, CSV, Excel, SQLite

## ğŸš€ Quick Start

```bash
# Install dependencies
py -m pip install -r requirements.txt

# Run the crawler
py crawler_app.py
```

## ï¿½ GitHub Deployment

Want to push this project to GitHub? See [`GITHUB_PUSH_INSTRUCTIONS.md`](GITHUB_PUSH_INSTRUCTIONS.md) for detailed steps including:
- Repository creation guide
- Ready-to-use descriptions
- Push commands and setup
- Recommended GitHub topics

## ï¿½ğŸ“¦ What's Included

**Single File:** `crawler_app.py`
- Data extraction engine
- Multi-format exporter
- Web crawler core
- Modern GUI

**Dependencies:**
- `requests`, `beautifulsoup4` - Web crawling
- `pandas`, `openpyxl` - Data processing & Excel
- `lxml` - Fast HTML parsing

## ğŸ’¡ Features

### Data Extraction
- ğŸ“‹ **Tables** - Automatic detection with pandas
- ğŸ·ï¸ **Metadata** - SEO tags, Open Graph
- ğŸ“Š **Structured Data** - JSON-LD extraction
- ğŸ“ **Text** - Clean paragraphs + word counts
- ğŸ”— **Links** - Internal/external categorization
- ğŸ“‘ **Lists** - Ordered and unordered
- ğŸ“„ **Forms** - Input field analysis

### Export Formats
| Format | Features |
|--------|----------|
| **Excel** | Multi-sheet workbooks (Overview + Tables) |
| **JSON** | Structured hierarchical data |
| **CSV** | Flat table for analysis |
| **SQLite** | Relational database with indexes |

### UI Layout

```
Left Panel (Controls)    Right Panel (Results)
â”œâ”€â”€ URL Input           â”œâ”€â”€ Live Stats (4 cards)
â”œâ”€â”€ Settings            â”œâ”€â”€ Progress Log
â”œâ”€â”€ Export Format       â””â”€â”€ Real-time Updates
â””â”€â”€ Action Buttons
```

## ğŸ“– Usage

1. **Extract**: Enter URL â†’ Configure settings â†’ Click START
2. **Monitor**: Watch live stats and progress log
3. **Export**: Select format â†’ Click EXPORT â†’ Files in `exports/`

## ğŸ¯ Use Cases

- Market research & competitor analysis
- SEO metadata collection
- Content aggregation
- Data science dataset creation
- Business intelligence

## ğŸ“Š Sample Output

### JSON Structure
```json
{
  "url": "...",
  "metadata": {"title": "...", "description": "..."},
  "tables": [{"rows": 10, "cols": 3, "data": [...]}],
  "text": {"paragraphs": [...], "word_count": 1234},
  "links": {"internal": [...], "external": [...]}
}
```

### Excel Sheets
- **Overview**: URLs, titles, word counts, table/link counts
- **Tables**: Extracted table data with source URLs

## ğŸ—ï¸ Architecture

**Self-Contained Design:**
```
crawler_app.py (700+ lines)
â”œâ”€â”€ DataExtractor        â†’ HTML parsing & extraction
â”œâ”€â”€ DataExporter         â†’ Multi-format export engine
â”œâ”€â”€ IndustrialCrawler   â†’ Core crawling logic
â””â”€â”€ IndustrialCrawlerGUI â†’ Modern UI (Tkinter)
```

**No External Files Needed!**

## âš¡ What's New

### v2.0 - Consolidated Edition
âœ… Merged all modules into single file  
âœ… Redesigned UI with two-panel layout  
âœ… Enhanced data extraction (forms, lists, headings)  
âœ… Live statistics dashboard  
âœ… Improved error handling  
âœ… Better logging system  

### Legacy Files (Archived)
The `Layout_extracter &Security_cheecker for web/` folder contains the original modular files:
- ğŸ“¦ `polite_crawler_enhanced.py` - Original polite crawler (merged)
- ğŸ“¦ `security_crawler_enhanced.py` - Original security crawler (merged)

These files are **no longer required** to run the application. All functionality has been consolidated into `crawler_app.py`.


## ğŸ“ Project Structure

```
wflow/
â”œâ”€â”€ crawler_app.py                            # Main application (all-in-one) â­
â”œâ”€â”€ requirements.txt                          # Dependencies
â”œâ”€â”€ exports/                                  # Exported data files
â”œâ”€â”€ industrial_crawler.log                    # Application logs
â”œâ”€â”€ README.md                                 # This file
â”œâ”€â”€ GITHUB_PUSH_INSTRUCTIONS.md               # GitHub deployment guide
â”œâ”€â”€ .github-repo-description.txt              # Repository descriptions
â””â”€â”€ Layout_extracter &Security_cheecker for web/  # Legacy files (archived)
    â”œâ”€â”€ polite_crawler_enhanced.py            #   Original polite crawler
    â””â”€â”€ security_crawler_enhanced.py          #   Original security crawler
```

> **Note:** The files in `Layout_extracter &Security_cheecker for web/` are the original modules that have been **merged into `crawler_app.py`**. They are kept for reference but are no longer needed to run the application.

## ğŸ”§ Configuration

Adjust settings in the UI:
- **Max Pages**: 1-500 pages per crawl
- **Delay**: 0.1-10 seconds between requests
- **Export Format**: Choose from 4 formats

## ğŸ“ Logging

All activity logged to: `industrial_crawler.log`

Format: `timestamp - level - message`

## ğŸš§ Roadmap

- [ ] Session resume capability
- [ ] Proxy rotation support
- [ ] JavaScript rendering (Selenium)
- [ ] Custom CSS selector builder
- [ ] Automatic pagination detection

## ğŸ“ Support

- Check `walkthrough.md` for detailed documentation
- See `GITHUB_PUSH_INSTRUCTIONS.md` for GitHub deployment help

---

**Built with Python â€¢ Tkinter â€¢ Pandas â€¢ BeautifulSoup**

**Version 2.0** - Consolidated Single-File Edition
